from langchain.tools import StructuredTool
from pydantic import BaseModel, Field

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import skew, kurtosis, spearmanr, chi2_contingency
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import silhouette_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor


from typing import Optional
from typing import List
import json

# ----------------------------------
# Schemas e fun√ß√µes para as ferramentas
# ----------------------------------

# Schema gen√©rico para fun√ß√µes que aceitam coluna opcional
class ColunaInput(BaseModel):
    coluna: str = Field(
        default=None,
        description="Nome da coluna a ser analisada. Se n√£o for fornecido, aplica em todas as colunas relevantes."
    )

# Schema vazio para fun√ß√µes sem argumentos
class SemArgs(BaseModel):
    pass


# Schema espec√≠fico para ferramenta de histograma
class HistogramInput(BaseModel):
    coluna: str = Field(..., description="Nome da coluna num√©rica para gerar o histograma.")
    min_bins: Optional[int] = Field(None, description="N√∫mero m√≠nimo de intervalos (bins).")
    max_bins: Optional[int] = Field(None, description="N√∫mero m√°ximo de intervalos (bins).")
    default_bins: Optional[int] = Field(None, description="N√∫mero de intervalos padr√£o.")
    steps: Optional[int] = Field(None, description="Incremento do slider.")
    edge_color: Optional[str] = Field(None, description="Cor da borda das barras.")
    title: Optional[str] = Field(None, description="T√≠tulo do gr√°fico.")
    xlabel: Optional[str] = Field(None, description="R√≥tulo do eixo X.")
    ylabel: Optional[str] = Field(None, description="R√≥tulo do eixo Y.")

# Schema espec√≠fico para ferramenta de an√°lise de outliers
class OutlierAnalyzerInput(BaseModel):
    coluna: str = Field(...,description="Nome da coluna num√©rica para an√°lise de outliers via IQR.")
    title: Optional[str] = Field(None,description="T√≠tulo opcional do boxplot (padr√£o: 'Boxplot de {coluna}').")
    ylabel: Optional[str] = Field(None,description="R√≥tulo opcional do eixo Y (padr√£o: nome da coluna).")
    iqr_multiplier: Optional[float] = Field(1.5,description="Multiplicador do IQR para definir limites de outliers (padr√£o 1.5).")
    incluir_resumo_estatistico: Optional[bool] = Field(True,description="Se verdadeiro, inclui Q1, Q3, IQR e propor√ß√£o de outliers no texto.")
    incluir_recomendacoes: Optional[bool] = Field(True,description="Se verdadeiro, inclui recomenda√ß√µes (remover, transformar, investigar).")
    remover_outliers: Optional[bool] = Field(False,description="Se verdadeiro, remove os outliers antes de gerar o boxplot e compara estat√≠sticas com a vers√£o normal.")
    transformar_log: Optional[bool] = Field(False,description="Se verdadeiro, aplica transforma√ß√£o logar√≠tmica (log1p) antes de gerar o boxplot e compara estat√≠sticas com a vers√£o normal.")
    winsorizar: Optional[bool] = Field(False,description="Se verdadeiro, aplica winsoriza√ß√£o (limita valores aos limites do IQR) antes de gerar o boxplot e compara estat√≠sticas com a vers√£o normal.")

# Schema espec√≠fico para ferramenta de scatterplot

class ScatterplotInput(BaseModel):
    coluna_x: str = Field(...,description="Nome da coluna num√©rica para o eixo X.")
    coluna_y: str = Field(...,description="Nome da coluna num√©rica para o eixo Y.")
    title: Optional[str] = Field(None,description="T√≠tulo opcional do scatterplot (padr√£o: 'Rela√ß√£o entre {coluna_x} e {coluna_y}').")
    xlabel: Optional[str] = Field(None,description="R√≥tulo opcional do eixo X (padr√£o: nome da coluna X).")
    ylabel: Optional[str] = Field(None,description="R√≥tulo opcional do eixo Y (padr√£o: nome da coluna Y).")

# Schema espec√≠fico para ferramenta de heatmap de clusters

class ClusterAnalyzerInput(BaseModel):
    max_clusters: Optional[int] = Field(6, description="N√∫mero m√°ximo de clusters a testar (padr√£o: 6).")
    random_state: Optional[int] = Field(42, description="Semente aleat√≥ria para reprodutibilidade (padr√£o: 42).")
    min_cluster_size: Optional[int] = Field(0, description="Tamanho m√≠nimo de observa√ß√µes por cluster (0 = n√£o aplica).")
    scaling: Optional[str] = Field("standard", description="M√©todo de normaliza√ß√£o: 'standard' (z-score), 'minmax' ou 'none'.")

# Schema espec√≠fico para ferramenta de tabela cruzada

class CrosstabAnalyzerInput(BaseModel):
    coluna_x: str = Field(..., description="Nome da coluna categ√≥rica para as linhas da tabela cruzada.")
    coluna_y: str = Field(..., description="Nome da coluna categ√≥rica para as colunas da tabela cruzada.")
    normalize: bool = Field(False, description="Se True, mostra propor√ß√µes em vez de contagens absolutas.")
    output: str = Field("table", description="Formato de sa√≠da desejado: 'table' para tabela ou 'heatmap' para visualiza√ß√£o gr√°fica.")
    max_table_dim: int = Field(10, description="N√∫mero m√°ximo de linhas/colunas para exibir como tabela. Acima disso, for√ßa heatmap.")

# Schema espec√≠fico para ferramenta de matriz de correla√ß√£o

class CorrelationMatrixInput(BaseModel):
    method: str = Field("pearson", description="M√©todo de correla√ß√£o: 'pearson', 'spearman' ou 'kendall'.")
    top_n: int = Field(3, description="N√∫mero de pares mais correlacionados a destacar na interpreta√ß√£o.")

# Schema espec√≠fico para ferramenta de an√°lise de tend√™ncias temporais

class TemporalTrendsInput(BaseModel):
    coluna_tempo: str = Field(..., description="Nome da coluna temporal (datetime).")
    variaveis: List[str] = Field(..., description="Lista de vari√°veis num√©ricas a analisar.")
    freq: str = Field("D", description="Frequ√™ncia de agrega√ß√£o: 'D' (di√°rio), 'M' (mensal), 'Y' (anual).")
    agg: str = Field("mean", description="Fun√ß√£o de agrega√ß√£o: 'mean', 'sum', etc.")

# Schema espec√≠fico para ferramenta de import√¢ncia de features

class FeatureImportanceInput(BaseModel):
    target: str = Field(..., description="Nome da vari√°vel alvo (dependente).")
    features: List[str] = Field(..., description="Lista de vari√°veis independentes (num√©ricas).")
    method: str = Field("linear", description="M√©todo: 'linear' para regress√£o linear m√∫ltipla ou 'tree' para √°rvore de decis√£o.")
    max_depth: int = Field(4, description="Profundidade m√°xima da √°rvore (se method='tree').")
    random_state: int = Field(42, description="Semente aleat√≥ria para reprodutibilidade (se method='tree').")

# Schema espec√≠fico para ferramenta de frequ√™ncia de valores

class FrequenciaValoresInput(BaseModel):
    coluna: str = Field(..., description="Nome da coluna a ser analisada.")
    top_n: int = Field(5, description="N√∫mero de valores mais/menos frequentes a destacar.")

# -----------------------------------
# Fun√ß√µes das ferramentas
# -----------------------------------

def get_columns(*args, **kwargs):
    """Retorna a lista de colunas do DataFrame de amostra."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."
    return list(df.columns)

def get_summary(coluna: str = None, *args, **kwargs):
    """Resumo estat√≠stico do DataFrame inteiro ou de uma coluna espec√≠fica."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."
    if coluna and coluna in df.columns:
        return df[[coluna]].describe(include="all").to_dict()
    return df.describe(include="all").to_dict()

def get_central_tendency(coluna: str = None, *args, **kwargs):
    """Retorna m√©dia, mediana e moda de uma coluna num√©rica ou de todas as num√©ricas."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."

    medidas = {}

    # Se coluna espec√≠fica foi pedida
    if coluna:
        if coluna not in df.columns:
            return f"A coluna '{coluna}' n√£o existe."
        if not pd.api.types.is_numeric_dtype(df[coluna]):
            return f"A coluna '{coluna}' n√£o √© num√©rica. Use resumo_estatistico para analis√°-la."
        moda = df[coluna].mode().iloc[0] if not df[coluna].mode().empty else None
        medidas[coluna] = {
            "media": df[coluna].mean(),
            "mediana": df[coluna].median(),
            "moda": moda
        }
        return medidas

    # Caso contr√°rio, aplica em todas as num√©ricas
    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            moda = df[col].mode().iloc[0] if not df[col].mode().empty else None
            medidas[col] = {
                "media": df[col].mean(),
                "mediana": df[col].median(),
                "moda": moda
            }
    return medidas

def get_variability(coluna: str = None, *args, **kwargs):
    """Retorna desvio padr√£o e vari√¢ncia de uma coluna num√©rica ou de todas as num√©ricas."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."

    variab = {}

    if coluna:
        if coluna not in df.columns:
            return f"A coluna '{coluna}' n√£o existe."
        if not pd.api.types.is_numeric_dtype(df[coluna]):
            return f"A coluna '{coluna}' n√£o √© num√©rica. Use resumo_estatistico para analis√°-la."
        variab[coluna] = {
            "desvio_padrao": df[coluna].std(),
            "variancia": df[coluna].var()
        }
        return variab

    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            variab[col] = {
                "desvio_padrao": df[col].std(),
                "variancia": df[col].var()
            }
    return variab

def get_data_types(*args, **kwargs):
    """Retorna os tipos de dados de cada coluna, separando num√©ricas e categ√≥ricas."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."
    tipos = df.dtypes.apply(lambda x: str(x)).to_dict()
    numericos = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col])]
    categoricos = [col for col in df.columns if col not in numericos]
    return {
        "tipos": tipos,
        "numericos": numericos,
        "categoricos": categoricos
    }

def get_distributions(coluna: str = None, *args, **kwargs):
    """Retorna distribui√ß√µes: estat√≠sticas para num√©ricas, contagem para categ√≥ricas.
       Pode ser de uma coluna espec√≠fica ou de todas."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."

    distrib = {}

    if coluna:
        if coluna not in df.columns:
            return f"A coluna '{coluna}' n√£o existe."
        if pd.api.types.is_numeric_dtype(df[coluna]):
            distrib[coluna] = df[coluna].describe().to_dict()
        else:
            distrib[coluna] = df[coluna].value_counts().head(10).to_dict()
        return distrib

    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            distrib[col] = df[col].describe().to_dict()
        else:
            distrib[col] = df[col].value_counts().head(10).to_dict()
    return distrib

def get_ranges(coluna: str = None, *args, **kwargs):
    """Retorna intervalo (m√≠nimo e m√°ximo) de uma coluna num√©rica ou de todas as num√©ricas."""
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return "Nenhuma amostra dispon√≠vel."

    ranges = {}

    if coluna:
        if coluna not in df.columns:
            return f"A coluna '{coluna}' n√£o existe."
        if not pd.api.types.is_numeric_dtype(df[coluna]):
            return f"A coluna '{coluna}' n√£o √© num√©rica. Use resumo_estatistico para analis√°-la."
        ranges[coluna] = {
            "min": df[coluna].min(),
            "max": df[coluna].max()
        }
        return ranges

    for col in df.columns:
        if pd.api.types.is_numeric_dtype(df[col]):
            ranges[col] = {
                "min": df[col].min(),
                "max": df[col].max()
            }
    return ranges

# Fun√ß√µes auxiliares para sugest√µes de bins
def bins_sturges(data):
    n = len(data)
    return int(np.ceil(np.log2(n) + 1))

def bins_freedman_diaconis(data):
    q75, q25 = np.percentile(data, [75, 25])
    iqr = q75 - q25
    bin_width = 2 * iqr / (len(data) ** (1/3))
    if bin_width == 0:
        return 10  # fallback
    return int(np.ceil((data.max() - data.min()) / bin_width))


def bins_sturges(data):
    n = len(data)
    return int(np.ceil(np.log2(n) + 1))

def bins_freedman_diaconis(data):
    q75, q25 = np.percentile(data, [75, 25])
    iqr = q75 - q25
    bin_width = 2 * iqr / (len(data) ** (1/3))
    if bin_width == 0:
        return 10  # fallback
    return int(np.ceil((data.max() - data.min()) / bin_width))

# ---------------------------------
# Fun√ß√µes auxiliares para histograma
# ---------------------------------
def analisar_distribuicao(coluna: str, df: pd.DataFrame) -> dict:
    if coluna not in df.columns:
        return {"status": "error", "mensagem": f"A coluna '{coluna}' n√£o existe."}

    if not pd.api.types.is_numeric_dtype(df[coluna]):
        return {"status": "error", "mensagem": f"A coluna '{coluna}' n√£o √© num√©rica."}

    serie = df[coluna].dropna()

    resultados = {
        "coluna": coluna,
        "n": len(serie),
        "min": float(serie.min()),
        "max": float(serie.max()),
        "media": float(serie.mean()),
        "mediana": float(serie.median()),
        "moda": float(serie.mode().iloc[0]) if not serie.mode().empty else None,
        "desvio_padrao": float(serie.std()),
        "variancia": float(serie.var()),
        "q1": float(serie.quantile(0.25)),
        "q3": float(serie.quantile(0.75)),
        "iqr": float(serie.quantile(0.75) - serie.quantile(0.25)),
        "assimetria": float(skew(serie)),
        "curtose": float(kurtosis(serie))
    }
    return resultados

def interpretar_distribuicao(stats: dict) -> str:
    if stats.get("status") == "error":
        return stats["mensagem"]

    media, mediana, skewness, curtose = stats["media"], stats["mediana"], stats["assimetria"], stats["curtose"]

    conclusoes = []

    # Simetria
    if abs(media - mediana) < 0.05 * stats["desvio_padrao"]:
        conclusoes.append("A distribui√ß√£o √© aproximadamente sim√©trica.")
    elif media > mediana:
        conclusoes.append("A distribui√ß√£o √© assim√©trica √† direita (cauda longa para valores altos).")
    else:
        conclusoes.append("A distribui√ß√£o √© assim√©trica √† esquerda (cauda longa para valores baixos).")

    # Dispers√£o
    if stats["desvio_padrao"] > 0.5 * (stats["max"] - stats["min"]):
        conclusoes.append("Os dados s√£o bastante dispersos.")
    else:
        conclusoes.append("Os dados est√£o relativamente concentrados.")

    # Curtose
    if curtose > 0:
        conclusoes.append("A distribui√ß√£o √© mais pontuda que a normal (caudas pesadas).")
    else:
        conclusoes.append("A distribui√ß√£o √© mais achatada que a normal.")

    # Outliers potenciais
    limite_inf = stats["q1"] - 1.5 * stats["iqr"]
    limite_sup = stats["q3"] + 1.5 * stats["iqr"]
    if stats["min"] < limite_inf or stats["max"] > limite_sup:
        conclusoes.append("H√° ind√≠cios de outliers nos extremos.")

    return " ".join(conclusoes)

# -----------------------------------
# Fun√ß√£o principal para histograma
# -----------------------------------
def histogram_maker(
    coluna: str,
    min_bins: int = None,
    max_bins: int = None,
    default_bins: int = None,
    steps: int = None,
    edge_color: str = None,
    title: str = None,
    xlabel: str = None,
    ylabel: str = None
) -> str:
    """Gera um dicion√°rio de informa√ß√µes para plotar um histograma pela interface Streamlit."""

    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel para an√°lise.", "status": "error"})

    if coluna not in df.columns:
        return json.dumps({"mensagem": f"A coluna '{coluna}' n√£o existe na amostra.", "status": "error"})

    if not pd.api.types.is_numeric_dtype(df[coluna]):
        return json.dumps({"mensagem": f"A coluna '{coluna}' n√£o √© num√©rica e n√£o pode ser usada para histograma.", "status": "error"})
    
    data = df[coluna].dropna().values
    n = int(len(data))
    if n == 0:
        return json.dumps({"mensagem": f"A coluna '{coluna}' n√£o possui dados v√°lidos para histograma.", "status": "error"})

    # Sugest√µes autom√°ticas
    sturges_raw = bins_sturges(data) if n > 1 else 1
    fd_raw = bins_freedman_diaconis(data) if n > 1 else 1

    try:
        sturges_bins = max(1, min(n, int(round(sturges_raw))))
    except Exception:
        sturges_bins = 1
    try:
        fd_bins = max(1, min(n, int(round(fd_raw))))
    except Exception:
        fd_bins = sturges_bins

    recomendacao_max = max(sturges_bins, fd_bins)
    recomendacao_min = min(sturges_bins, fd_bins)

    # Sanidade: n√£o ter mais bins que pontos
    cap_max = max(1, n)

    # Defaults inteligentes
    min_bins = int(min_bins) if min_bins is not None else max(2 if n >= 2 else 1, recomendacao_min)
    max_bins = int(max_bins) if max_bins is not None else max(50, recomendacao_max)
    max_bins = min(max_bins, cap_max)

    # Ajuste de seguran√ßa: garante que min_bins n√£o ultrapasse max_bins
    min_bins = min(min_bins, max_bins)

    # Default
    if default_bins is None:
        default_bins = sturges_bins if n < 10 else (fd_bins if fd_bins > sturges_bins else sturges_bins)
    default_bins = max(min_bins, min(default_bins, max_bins))

    # Passo din√¢mico
    if steps is None:
        span = max_bins - min_bins
        steps = 1 if span <= 50 else max(1, int(round(span / 50)))
    else:
        steps = max(1, int(steps))

    edge_color = edge_color or "black"
    title = title or f"Histograma de {coluna}"
    xlabel = xlabel or coluna
    ylabel = ylabel or "Frequ√™ncia"
    
    # An√°lise descritiva da coluna
    stats = analisar_distribuicao(coluna, df)
    interpretacao = interpretar_distribuicao(stats)

    mensagem = (
        f"Interpreta√ß√£o da distribui√ß√£o da coluna '{coluna}':\n\n"
        f"{interpretacao}\n\n"
        f"üìä {title}\n\n"
        f"**Sugest√µes de intervalos:**\n"
        f"- Sturges: `{sturges_bins}`\n"
        f"- Freedman‚ÄìDiaconis: `{fd_bins}`\n"
        f"(n={n}; slider: {min_bins}‚Äì{max_bins}, passo {steps}, padr√£o {default_bins})"
    )

    hist_inst = {
        "mensagem": mensagem,
        "acao": "histograma",
        "args": {
            "coluna": coluna,
            "min_bins": min_bins,
            "max_bins": max_bins,
            "default_bins": default_bins,
            "steps": steps,
            "edge_color": edge_color,
            "title": title,
            "xlabel": xlabel,
            "ylabel": ylabel
        },
        "status": "success"
    }
    return json.dumps(hist_inst)

# -----------------------------------
# Fun√ß√µes auxiliares de detec√ß√£o e tratamento de outliers
# -----------------------------------

def calcular_outlier_stats(coluna: str, df: pd.DataFrame, iqr_multiplier: float = 1.5) -> dict:
    """Calcula estat√≠sticas de outliers via regra do IQR com multiplicador configur√°vel."""
    serie = df[coluna].dropna()
    q1 = serie.quantile(0.25)
    q3 = serie.quantile(0.75)
    iqr = q3 - q1
    limite_inf = q1 - iqr_multiplier * iqr
    limite_sup = q3 + iqr_multiplier * iqr

    outliers = serie[(serie < limite_inf) | (serie > limite_sup)]
    n_outliers = len(outliers)
    n_total = len(serie)

    return {
        "coluna": coluna,
        "n_total": n_total,
        "q1": float(q1),
        "q3": float(q3),
        "iqr": float(iqr),
        "limite_inf": float(limite_inf),
        "limite_sup": float(limite_sup),
        "n_outliers": n_outliers,
        "prop_outliers": n_outliers / n_total if n_total > 0 else 0.0,
        "media": float(serie.mean()) if n_total > 0 else None,
        "mediana": float(serie.median()) if n_total > 0 else None,
        "desvio_padrao": float(serie.std()) if n_total > 0 else None
    }

def interpretar_outliers(stats: dict, incluir_resumo_estatistico: bool = True, incluir_recomendacoes: bool = True) -> str:
    """Gera interpreta√ß√£o textual dos outliers detectados."""
    if stats["n_total"] == 0:
        return "A coluna n√£o possui dados v√°lidos para an√°lise de outliers."

    partes = []

    if stats["n_outliers"] == 0:
        partes.append(f"N√£o foram detectados valores at√≠picos na coluna '{stats['coluna']}'.")
    else:
        partes.append(
            f"Foram detectados {stats['n_outliers']} outliers "
            f"({stats['prop_outliers']:.1%} do total) na coluna '{stats['coluna']}'."
        )

    if incluir_resumo_estatistico:
        partes.append(
            f"(Q1={stats['q1']:.2f}, Q3={stats['q3']:.2f}, IQR={stats['iqr']:.2f}, "
            f"limite inferior={stats['limite_inf']:.2f}, limite superior={stats['limite_sup']:.2f})"
        )

    if stats["n_outliers"] > 0:
        if stats["prop_outliers"] > 0.2:
            partes.append("A propor√ß√£o de outliers √© alta, o que pode distorcer m√©dias e an√°lises.")
        elif stats["prop_outliers"] > 0.05:
            partes.append("H√° uma quantidade moderada de outliers, que pode afetar algumas m√©tricas.")
        else:
            partes.append("A quantidade de outliers √© pequena e provavelmente n√£o compromete a an√°lise.")

    if incluir_recomendacoes and stats["n_outliers"] > 0:
        partes.append(
            "Esses valores podem ser investigados individualmente. "
            "Dependendo do contexto, podem ser removidos, transformados "
            "ou tratados como casos especiais."
        )

    return " ".join(partes)

# -----------------------------------
# Fun√ß√£o principal para an√°lise de outliers
# -----------------------------------

def outlier_analyzer(
    coluna: str,
    title: str = None,
    ylabel: str = None,
    iqr_multiplier: float = 1.5,
    incluir_resumo_estatistico: bool = True,
    incluir_recomendacoes: bool = True,
    remover_outliers: bool = False,
    transformar_log: bool = False,
    winsorizar: bool = False
) -> str:
    """Analisa outliers e, se solicitado, aplica transforma√ß√µes e compara com a situa√ß√£o normal."""

    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel para an√°lise.", "status": "error"})

    if coluna not in df.columns:
        return json.dumps({"mensagem": f"A coluna '{coluna}' n√£o existe na amostra.", "status": "error"})

    if not pd.api.types.is_numeric_dtype(df[coluna]):
        return json.dumps({"mensagem": f"A coluna '{coluna}' n√£o √© num√©rica e n√£o pode ser usada em boxplot.", "status": "error"})

    serie_normal = df[coluna].dropna()

    # Estat√≠sticas normais e interpreta√ß√£o
    stats_normal = calcular_outlier_stats(coluna, df, iqr_multiplier)
    interpretacao_normal = interpretar_outliers(stats_normal, incluir_resumo_estatistico, incluir_recomendacoes)

    # Determinar transforma√ß√£o escolhida (hierarquia)
    escolhas = [remover_outliers, winsorizar, transformar_log]
    if sum(escolhas) > 1:
        aviso = "‚ö†Ô∏è Mais de uma op√ß√£o de tratamento foi passada. Aplicando hierarquia: remover_outliers > winsorizar > transformar_log."
        if remover_outliers:
            modo = "remover_outliers"
        elif winsorizar:
            modo = "winsorizar"
        else:
            modo = "transformar_log"
    elif remover_outliers:
        modo = "remover_outliers"; aviso = ""
    elif winsorizar:
        modo = "winsorizar"; aviso = ""
    elif transformar_log:
        modo = "transformar_log"; aviso = ""
    else:
        modo = "normal"; aviso = ""

    # Aplicar transforma√ß√£o
    if modo == "remover_outliers":
        q1, q3 = serie_normal.quantile([0.25, 0.75])
        iqr = q3 - q1
        limite_inf = q1 - iqr_multiplier * iqr
        limite_sup = q3 + iqr_multiplier * iqr
        serie_transf = serie_normal[(serie_normal >= limite_inf) & (serie_normal <= limite_sup)]
    elif modo == "winsorizar":
        q1, q3 = serie_normal.quantile([0.25, 0.75])
        iqr = q3 - q1
        limite_inf = q1 - iqr_multiplier * iqr
        limite_sup = q3 + iqr_multiplier * iqr
        serie_transf = serie_normal.clip(lower=limite_inf, upper=limite_sup)
    elif modo == "transformar_log":
        # Log seguro: somente valores >= 0. Para negativos, sugere-se investigar/ajustar antes.
        serie_transf = np.log1p(serie_normal[serie_normal >= 0])
    else:
        serie_transf = serie_normal

    # Estat√≠sticas transformadas (para compara√ß√£o)
    stats_transf = {
        "media": float(serie_transf.mean()) if len(serie_transf) > 0 else None,
        "mediana": float(serie_transf.median()) if len(serie_transf) > 0 else None,
        "desvio_padrao": float(serie_transf.std()) if len(serie_transf) > 0 else None
    }

    # Mensagem comparativa
    mensagem = (
        f"An√°lise de outliers da coluna '{coluna}':\n\n"
        f"{interpretacao_normal}\n\n"
    )
    if modo != "normal":
        mensagem += (
            f"Compara√ß√£o com tratamento '{modo}':\n"
            f"- M√©dia normal: {stats_normal['media']:.2f} ‚Üí ap√≥s: {stats_transf['media']:.2f}\n"
            f"- Mediana normal: {stats_normal['mediana']:.2f} ‚Üí ap√≥s: {stats_transf['mediana']:.2f}\n"
            f"- Desvio padr√£o normal: {stats_normal['desvio_padrao']:.2f} ‚Üí ap√≥s: {stats_transf['desvio_padrao']:.2f}\n\n"
            f"üìä Boxplot exibido corresponde √† situa√ß√£o '{modo}'.\n"
        )
    else:
        mensagem += "üìä Boxplot exibido corresponde √† situa√ß√£o normal.\n"

    if aviso:
        mensagem = aviso + "\n\n" + mensagem

    # Montagem do dicion√°rio de instru√ß√µes para o render
    title = title or f"Boxplot de {coluna}" + (f" ({modo})" if modo != "normal" else "")
    ylabel = ylabel or coluna

    box_inst = {
        "mensagem": mensagem,
        "acao": "boxplot",
        "args": {
            "coluna": coluna,
            "title": title,
            "ylabel": ylabel
        },
        "status": "success"
    }
    return json.dumps(box_inst)

# -----------------------------------
# Fun√ß√£o auxiliar para scatterplot
# -----------------------------------

def calcular_correlacoes(df: pd.DataFrame, coluna_x: str, coluna_y: str) -> dict:
    """Calcula correla√ß√µes de Pearson e Spearman entre duas colunas num√©ricas."""
    serie_x = df[coluna_x].dropna()
    serie_y = df[coluna_y].dropna()
    serie_x, serie_y = serie_x.align(serie_y, join="inner")

    pearson = serie_x.corr(serie_y, method="pearson")
    spearman, _ = spearmanr(serie_x, serie_y)

    return {
        "pearson": float(pearson),
        "spearman": float(spearman),
        "n": len(serie_x)
    }

def interpretar_correlacoes(corrs: dict, coluna_x: str, coluna_y: str) -> str:
    """Gera interpreta√ß√£o textual das correla√ß√µes calculadas."""
    def qualifica(valor: float) -> str:
        v = abs(valor)
        if v < 0.2: return "muito fraca"
        elif v < 0.4: return "fraca"
        elif v < 0.6: return "moderada"
        elif v < 0.8: return "forte"
        else: return "muito forte"

    partes = []
    partes.append(
        f"A correla√ß√£o de Pearson entre '{coluna_x}' e '{coluna_y}' √© {corrs['pearson']:.2f}, "
        f"com base em {corrs['n']} observa√ß√µes."
    )
    partes.append(f"A correla√ß√£o de Spearman √© {corrs['spearman']:.2f}.")
    partes.append(
        f"Isso indica uma rela√ß√£o {qualifica(corrs['pearson'])} (linear) "
        f"e {qualifica(corrs['spearman'])} (monot√¥nica)."
    )
    return " ".join(partes)

def calcular_diagnosticos(df: pd.DataFrame, coluna_x: str, coluna_y: str) -> dict:
    serie_x = df[coluna_x].dropna()
    serie_y = df[coluna_y].dropna()
    serie_x, serie_y = serie_x.align(serie_y, join="inner")

    resultados = {}

    # Outliers bivariados via regress√£o linear simples (numpy)
    coef = np.polyfit(serie_x, serie_y, 1)  # coef[0]=inclina√ß√£o, coef[1]=intercepto
    pred = np.polyval(coef, serie_x)
    residuos = serie_y - pred
    outliers = np.sum(np.abs(residuos) > 3 * residuos.std())
    resultados["outliers_bivariados"] = int(outliers)

    # Heterocedasticidade: vari√¢ncia condicional em quantis de X
    grupos = pd.qcut(serie_x, q=4, duplicates="drop")
    variancias = serie_y.groupby(grupos).var()
    resultados["heterocedasticidade"] = variancias.max() > 2 * variancias.min()

    return resultados

def interpretar_diagnosticos(diag: dict) -> str:
    """Interpreta os diagn√≥sticos adicionais do scatterplot."""
    partes = []
    if diag["outliers_bivariados"] > 0:
        partes.append(f"Foram detectados {diag['outliers_bivariados']} pontos fora do padr√£o esperado (outliers bivariados).")
    if diag["heterocedasticidade"]:
        partes.append("A dispers√£o dos pontos varia conforme X (heterocedasticidade).")
    if not partes:
        partes.append("N√£o foram detectados problemas relevantes de outliers bivariados ou heterocedasticidade.")
    return " ".join(partes)

def interpretar_inclinacao(corr: float, coluna_x: str, coluna_y: str) -> str:
    if corr > 0:
        return f"A rela√ß√£o entre '{coluna_x}' e '{coluna_y}' √© positiva: √† medida que {coluna_x} aumenta, {coluna_y} tende a aumentar."
    elif corr < 0:
        return f"A rela√ß√£o entre '{coluna_x}' e '{coluna_y}' √© negativa: √† medida que {coluna_x} aumenta, {coluna_y} tende a diminuir."
    else:
        return f"N√£o h√° inclina√ß√£o clara entre '{coluna_x}' e '{coluna_y}'."

def detectar_clusters(df: pd.DataFrame, coluna_x: str, coluna_y: str, max_clusters: int = 4) -> str:
    X = df[[coluna_x, coluna_y]].dropna().to_numpy()
    if len(X) < 10:
        return "Poucos dados para avaliar clusters."
    
    # Testa de 2 at√© max_clusters
    melhor_k = None
    melhor_inercia = None
    for k in range(2, max_clusters+1):
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
        kmeans.fit(X)
        if melhor_inercia is None or kmeans.inertia_ < melhor_inercia * 0.9:
            melhor_k = k
            melhor_inercia = kmeans.inertia_
    
    if melhor_k and melhor_k > 1:
        return f"Foram identificados aproximadamente {melhor_k} agrupamentos distintos de pontos (clusters)."
    else:
        return "N√£o foram identificados agrupamentos claros de pontos."

# -----------------------------------
# Fun√ß√£o principal para scatterplot
# -----------------------------------

def scatterplot_maker(
    coluna_x: str,
    coluna_y: str,
    title: str = None,
    xlabel: str = None,
    ylabel: str = None
) -> str:
    """Gera instru√ß√µes para plotar um scatterplot e inclui interpreta√ß√£o estat√≠stica da rela√ß√£o."""

    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    for col in [coluna_x, coluna_y]:
        if col not in df.columns:
            return json.dumps({"mensagem": f"A coluna '{col}' n√£o existe na amostra.", "status": "error"})
        if not pd.api.types.is_numeric_dtype(df[col]):
            return json.dumps({"mensagem": f"A coluna '{col}' n√£o √© num√©rica e n√£o pode ser usada em scatterplot.", "status": "error"})

    # Calcular correla√ß√µes e diagn√≥sticos
    corrs = calcular_correlacoes(df, coluna_x, coluna_y)
    interpretacao_corr = interpretar_correlacoes(corrs, coluna_x, coluna_y)

    interpretacao_inclinacao = interpretar_inclinacao(corrs["pearson"], coluna_x, coluna_y)
    interpretacao_clusters = detectar_clusters(df, coluna_x, coluna_y)

    diag = calcular_diagnosticos(df, coluna_x, coluna_y)
    interpretacao_diag = interpretar_diagnosticos(diag)

    # Defini√ß√µes padr√£o
    title = title or f"Rela√ß√£o entre {coluna_x} e {coluna_y}"
    xlabel = xlabel or coluna_x
    ylabel = ylabel or coluna_y

    mensagem = (
    f"Scatterplot mostrando a rela√ß√£o entre '{coluna_x}' e '{coluna_y}'.\n\n"
    f"{interpretacao_corr}\n\n"
    f"{interpretacao_inclinacao}\n\n"
    f"{interpretacao_diag}\n\n"
    f"{interpretacao_clusters}\n\n"
    f"üìä O gr√°fico abaixo ilustra essa rela√ß√£o."
    )

    scatter_inst = {
        "mensagem": mensagem,
        "acao": "scatterplot",
        "args": {
            "coluna_x": coluna_x,
            "coluna_y": coluna_y,
            "title": title,
            "xlabel": xlabel,
            "ylabel": ylabel
        },
        "status": "success"
    }
    return json.dumps(scatter_inst)


# -----------------------------------
# Fun√ß√µes auxiliares para an√°lise de clusters
# -----------------------------------

def interpretar_clusters(medias: pd.DataFrame, df_clusters: pd.DataFrame, min_cluster_size: int) -> str:
    interpretacoes = []

    # 1. Vari√°veis que mais diferenciam clusters
    variacoes = (medias.max() - medias.min()).sort_values(ascending=False)
    top_vars = variacoes.head(3).index.tolist()
    interpretacoes.append(
        f"As vari√°veis que mais diferenciam os clusters s√£o: {', '.join(top_vars)}."
    )

    # 2. Perfis m√©dios por cluster
    medias_globais = medias.mean()
    for cluster_id, row in medias.iterrows():
        acima = [col for col in medias.columns if row[col] > medias_globais[col]]
        abaixo = [col for col in medias.columns if row[col] < medias_globais[col]]
        interpretacoes.append(
            f"O Cluster {cluster_id} apresenta valores acima da m√©dia em {', '.join(acima) or 'nenhuma vari√°vel'} "
            f"e abaixo da m√©dia em {', '.join(abaixo) or 'nenhuma vari√°vel'}."
        )

    # 3. Clusters semelhantes ou distintos
    distancias = medias.apply(lambda x: np.linalg.norm(x - medias.mean()), axis=1)
    mais_proximos = distancias.sort_values().index.tolist()
    if len(mais_proximos) >= 2:
        interpretacoes.append(
            f"Os clusters {mais_proximos[0]} e {mais_proximos[1]} possuem perfis m√©dios bastante semelhantes."
        )

    # 4. Identifica√ß√£o de outliers coletivos
    counts = df_clusters["cluster"].value_counts()
    clusters_pequenos = counts[counts < max(min_cluster_size, 5)].index.tolist()
    if clusters_pequenos:
        interpretacoes.append(
            f"Os clusters {', '.join(map(str, clusters_pequenos))} possuem poucas observa√ß√µes e podem representar grupos at√≠picos ou ru√≠do."
        )

    return "\n\n".join(interpretacoes)

# -----------------------------------
# Fun√ß√£o principal para an√°lise de clusters
# -----------------------------------

def cluster_analyzer(
    max_clusters: int = 6,
    random_state: int = 42,
    min_cluster_size: int = 0,
    scaling: str = "standard"
) -> str:
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    # Seleciona apenas colunas num√©ricas
    num_df = df.select_dtypes(include=[np.number]).dropna()
    if num_df.shape[1] < 2:
        return json.dumps({"mensagem": "Poucas vari√°veis num√©ricas para clustering.", "status": "error"})

    # Normaliza√ß√£o
    if scaling == "standard":
        scaler = StandardScaler()
        X = scaler.fit_transform(num_df)
    elif scaling == "minmax":
        scaler = MinMaxScaler()
        X = scaler.fit_transform(num_df)
    else:  # "none"
        X = num_df.to_numpy()

    # Busca melhor n√∫mero de clusters
    melhor_k, melhor_score, melhor_modelo = None, -1, None
    for k in range(2, min(max_clusters, len(X))):
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=random_state)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        if score > melhor_score:
            melhor_k, melhor_score, melhor_modelo = k, score, kmeans

    if melhor_modelo is None:
        return json.dumps({"mensagem": "N√£o foi poss√≠vel identificar clusters.", "status": "error"})

    labels = melhor_modelo.labels_
    df_clusters = num_df.copy()
    df_clusters["cluster"] = labels

    # Aplica filtro de tamanho m√≠nimo de cluster
    if min_cluster_size > 0:
        counts = df_clusters["cluster"].value_counts()
        clusters_validos = counts[counts >= min_cluster_size].index
        df_clusters = df_clusters[df_clusters["cluster"].isin(clusters_validos)]

    if df_clusters.empty:
        return json.dumps({"mensagem": "Todos os clusters foram descartados pelo filtro de tamanho m√≠nimo.", "status": "error"})

    # M√©dias por cluster
    medias = df_clusters.groupby("cluster").mean().round(2)

    # Interpreta√ß√£o enriquecida
    interpretacoes_extra = interpretar_clusters(medias, df_clusters, min_cluster_size)

    mensagem = (
        f"Foram identificados **{medias.shape[0]} clusters** nos dados, "
        f"com um √≠ndice de silhueta de {melhor_score:.2f}.\n\n"
        f"O heatmap abaixo mostra as m√©dias das vari√°veis num√©ricas em cada cluster.\n\n"
        f"{interpretacoes_extra}"
    )

    cluster_inst = {
        "mensagem": mensagem,
        "acao": "heatmap_clusters",
        "args": {
            "medias": medias.to_dict(),
            "variaveis": list(medias.columns),
            "clusters": list(medias.index)
        },
        "status": "success"
    }
    return json.dumps(cluster_inst)

# -----------------------------------------------
# Fun√ß√µes auxiliares para an√°lise de cross-tab
# -----------------------------------------------
def gerar_crosstab(df, coluna_x, coluna_y, normalize=False):
    """
    Gera a tabela cruzada e retorna tamb√©m estat√≠sticas √∫teis.
    """
    ct = pd.crosstab(df[coluna_x], df[coluna_y])
    if normalize:
        ct_norm = ct / ct.values.sum()
    else:
        ct_norm = None

    # Estat√≠sticas b√°sicas
    total = ct.values.sum()
    row_totals = ct.sum(axis=1).to_dict()
    col_totals = ct.sum(axis=0).to_dict()

    return {
        "crosstab": ct,
        "crosstab_norm": ct_norm,
        "total": int(total),
        "row_totals": row_totals,
        "col_totals": col_totals
    }

def calcular_associacao(ct: pd.DataFrame):
    """
    Calcula teste qui-quadrado para verificar associa√ß√£o entre vari√°veis.
    """
    chi2, p, dof, expected = chi2_contingency(ct)
    return {
        "chi2": chi2,
        "p_value": p,
        "graus_liberdade": dof,
        "significativo": p < 0.05
    }

def interpretar_crosstab(ct: pd.DataFrame, stats: dict, coluna_x: str, coluna_y: str):
    """
    Gera uma interpreta√ß√£o textual da tabela cruzada e do teste estat√≠stico.
    """
    insights = []

    # Categoria mais frequente
    max_cell = ct.stack().idxmax()
    max_value = ct.stack().max()
    insights.append(
        f"A combina√ß√£o mais frequente √© **{coluna_x}={max_cell[0]}** com **{coluna_y}={max_cell[1]}**, "
        f"ocorrendo {max_value} vezes."
    )

    # Categoria dominante em cada linha
    for row in ct.index:
        col_dominante = ct.loc[row].idxmax()
        val = ct.loc[row].max()
        insights.append(
            f"Na categoria **{coluna_x}={row}**, a maior concentra√ß√£o est√° em **{coluna_y}={col_dominante}** ({val} ocorr√™ncias)."
        )

    # Teste de associa√ß√£o
    if stats["significativo"]:
        insights.append(
            f"O teste qui-quadrado indica associa√ß√£o significativa entre **{coluna_x}** e **{coluna_y}** "
            f"(œá¬≤={stats['chi2']:.2f}, p={stats['p_value']:.4f})."
        )
    else:
        insights.append(
            f"O teste qui-quadrado n√£o encontrou associa√ß√£o estatisticamente significativa "
            f"entre **{coluna_x}** e **{coluna_y}** (p={stats['p_value']:.4f})."
        )

    return "\n".join(insights)

# -------------------------------
# Fun√ß√£o principal para analise de cross-tab
# -------------------------------

def crosstab_analyzer(
    coluna_x: str,
    coluna_y: str,
    normalize: bool = False,
    output: str = "table",
    max_table_dim: int = 10  # limite para renderizar tabela (linhas e colunas)
) -> str:
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    if coluna_x not in df.columns or coluna_y not in df.columns:
        return json.dumps({"mensagem": f"Colunas {coluna_x} ou {coluna_y} n√£o encontradas.", "status": "error"})

    # Checagem de cardinalidade para decidir renderiza√ß√£o
    n_rows = df[coluna_x].nunique(dropna=False)
    n_cols = df[coluna_y].nunique(dropna=False)

    # Se tabela for muito grande, for√ßa heatmap para evitar mandar muitos dados
    render_output = output
    if output == "table" and (n_rows > max_table_dim or n_cols > max_table_dim):
        render_output = "heatmap"

    # Gera crosstab e interpreta√ß√µes
    ct_info = gerar_crosstab(df, coluna_x, coluna_y, normalize)
    stats = calcular_associacao(ct_info["crosstab"])
    interpretacao = interpretar_crosstab(ct_info["crosstab"], stats, coluna_x, coluna_y)

    # Mensagem final j√° com interpreta√ß√£o embutida
    mensagem = (
        f"Tabela cruzada entre **{coluna_x}** e **{coluna_y}** "
        f"{'(valores normalizados)' if normalize else '(contagens absolutas)'}."
        f"{' (Tabela grande: exibida como heatmap.)' if render_output == 'heatmap' and output == 'table' else ''}\n\n"
        f"**Interpreta√ß√£o autom√°tica:**\n{interpretacao}"
    )

    result = {
        "mensagem": mensagem,
        "acao": "crosstab",  # ploter √∫nico
        "args": {
            "coluna_x": coluna_x,
            "coluna_y": coluna_y,
            "normalize": normalize,
            "output": render_output,
            "max_table_dim": max_table_dim,
            "shape_hint": {"rows": n_rows, "cols": n_cols}
        },
        "status": "success"
    }
    return json.dumps(result)

# -----------------------------------
# Fun√ß√µes auxiliares para matriz de correla√ß√£o
# -----------------------------------

def calcular_matriz_correlacao(df: pd.DataFrame, metodo: str = "pearson") -> pd.DataFrame:
    """
    Calcula a matriz de correla√ß√£o entre vari√°veis num√©ricas.
    """
    num_df = df.select_dtypes(include=np.number)
    if num_df.empty:
        return pd.DataFrame()
    return num_df.corr(method=metodo)

def extrair_top_correlacoes(corr_matrix: pd.DataFrame, top_n: int = 5) -> list:
    """
    Extrai os top_n pares de vari√°veis mais correlacionados (positivos e negativos).
    Retorna lista de tuplas: (col1, col2, valor).
    """
    corr_pairs = (
        corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        .stack()
        .reset_index()
    )
    corr_pairs.columns = ["var1", "var2", "correlacao"]
    corr_pairs["abs_corr"] = corr_pairs["correlacao"].abs()
    top_pairs = corr_pairs.sort_values("abs_corr", ascending=False).head(top_n)
    return list(top_pairs[["var1", "var2", "correlacao"]].itertuples(index=False, name=None))

def interpretar_correlacoes_matrix(corr_matrix: pd.DataFrame, top_n: int = 5) -> str:
    """
    Gera interpreta√ß√£o textual da matriz de correla√ß√£o.
    """
    if corr_matrix.empty:
        return "N√£o foi poss√≠vel calcular correla√ß√µes, pois n√£o h√° vari√°veis num√©ricas."

    top_pairs = extrair_top_correlacoes(corr_matrix, top_n=top_n)

    mensagens = ["An√°lise de correla√ß√£o entre vari√°veis num√©ricas:"]
    for var1, var2, corr in top_pairs:
        if corr > 0.7:
            intensidade = "forte positiva"
        elif corr > 0.4:
            intensidade = "moderada positiva"
        elif corr > 0.2:
            intensidade = "fraca positiva"
        elif corr < -0.7:
            intensidade = "forte negativa"
        elif corr < -0.4:
            intensidade = "moderada negativa"
        elif corr < -0.2:
            intensidade = "fraca negativa"
        else:
            intensidade = "muito fraca ou inexistente"

        mensagens.append(
            f"- **{var1}** e **{var2}** apresentam correla√ß√£o {intensidade} ({corr:.2f})."
        )

    return "\n".join(mensagens)


# -----------------------------------
# Fun√ß√£o principal para matriz de correla√ß√£o
# -----------------------------------
def correlation_matrix(method: str = "pearson", top_n: int = 3) -> str:
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    num_df = df.select_dtypes(include="number")
    if num_df.shape[1] < 2:
        return json.dumps({"mensagem": "N√£o h√° colunas num√©ricas suficientes para calcular correla√ß√£o.", "status": "error"})

    corr = num_df.corr(method=method)

    # Identificar pares mais fortes
    corr_pairs = (
        corr.where(~np.tril(np.ones(corr.shape)).astype(bool))
        .stack()
        .sort_values(key=lambda x: x.abs(), ascending=False)
    )
    top_pairs = corr_pairs.head(top_n)


    # Interpreta√ß√£o textual usando fun√ß√£o auxiliar
    mensagem = interpretar_correlacoes_matrix(corr, top_n=top_n)


    result = {
        "mensagem": mensagem,
        "acao": "correlation_matrix",
        "args": {
            "method": method  # üëà s√≥ o necess√°rio para o ploter recalcular
        },
        "status": "success"
    }
    return json.dumps(result)

# -----------------------------------
# Fun√ß√£o auxiliar para tend√™ncias temporais
# -----------------------------------

def calcular_delta_inicial_final(serie: pd.Series) -> tuple:
    """
    Calcula a diferen√ßa entre o valor inicial e final de uma s√©rie temporal.
    Retorna (valor_inicial, valor_final, delta).
    """
    serie = serie.dropna()
    if serie.shape[0] < 2:
        return None, None, None
    return serie.iloc[0], serie.iloc[-1], serie.iloc[-1] - serie.iloc[0]


def interpretar_tendencia_variavel(serie: pd.Series, nome_coluna: str) -> str:
    """
    Gera interpreta√ß√£o textual para uma vari√°vel temporal.
    """
    inicial, final, delta = calcular_delta_inicial_final(serie)
    if inicial is None:
        return f"- A vari√°vel **{nome_coluna}** n√£o possui dados suficientes para an√°lise temporal."
    
    if delta > 0:
        direcao = "aumentou"
    elif delta < 0:
        direcao = "diminuiu"
    else:
        direcao = "permaneceu est√°vel"

    return f"- A vari√°vel **{nome_coluna}** {direcao} de {inicial:.2f} para {final:.2f}."


def interpretar_tendencias_temporais(ts: pd.DataFrame, freq: str, agg: str) -> str:
    """
    Gera interpreta√ß√£o textual para m√∫ltiplas vari√°veis temporais.
    """
    mensagens = [f"Evolu√ß√£o temporal das vari√°veis {', '.join(ts.columns)} agregadas por {freq} usando {agg}:"]
    for col in ts.columns:
        mensagens.append(interpretar_tendencia_variavel(ts[col], col))
    return "\n".join(mensagens)

# -----------------------------------
# Funcao principal para tend√™ncias temporais
# -----------------------------------

def temporal_trends(
    coluna_tempo: str,
    variaveis: list,
    freq: str = "D",
    agg: str = "mean"
) -> str:
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    if coluna_tempo not in df.columns:
        return json.dumps({"mensagem": f"Coluna temporal {coluna_tempo} n√£o encontrada.", "status": "error"})

    # Converter para datetime
    df[coluna_tempo] = pd.to_datetime(df[coluna_tempo], errors="coerce")
    if df[coluna_tempo].isna().all():
        return json.dumps({"mensagem": f"A coluna {coluna_tempo} n√£o cont√©m valores de data v√°lidos.", "status": "error"})

    # Selecionar vari√°veis num√©ricas v√°lidas
    num_cols = [c for c in variaveis if c in df.select_dtypes(include="number").columns]
    if not num_cols:
        return json.dumps({"mensagem": "Nenhuma vari√°vel num√©rica v√°lida selecionada.", "status": "error"})

    # Agrega√ß√£o temporal
    ts = df.set_index(coluna_tempo).resample(freq)[num_cols].agg(agg)

    # üëâ Interpreta√ß√£o textual usando fun√ß√£o auxiliar
    mensagem = interpretar_tendencias_temporais(ts, freq, agg)

    result = {
        "mensagem": mensagem,
        "acao": "temporal_trends",
        "args": {
            "coluna_tempo": coluna_tempo,
            "variaveis": num_cols,
            "freq": freq,
            "agg": agg
        },
        "status": "success"
    }
    return json.dumps(result)

# -----------------------------------
# Fun√ß√µes auxiliares para an√°lise de feature importance
# -----------------------------------
def interpretar_feature_importance(importances: list, target: str, method: str) -> str:
    """
    Gera interpreta√ß√£o textual a partir da lista de import√¢ncias.
    - importances: lista de tuplas (vari√°vel, import√¢ncia)
    - target: vari√°vel alvo
    - method: m√©todo usado ('linear' ou 'tree')
    """
    if not importances:
        return f"N√£o foi poss√≠vel calcular import√¢ncias para prever **{target}**."

    mensagens = [f"An√°lise de import√¢ncia de vari√°veis para prever **{target}** usando m√©todo **{method}**:"]

    # Destacar a vari√°vel mais importante
    mais_importante, maior_valor = importances[0]
    mensagens.append(f"A vari√°vel mais influente foi **{mais_importante}** (import√¢ncia {maior_valor:.2f}).")

    # Listar todas em ordem
    for feat, imp in importances:
        mensagens.append(f"- **{feat}**: import√¢ncia relativa {imp:.2f}")

    return "\n".join(mensagens)


# -----------------------------------
# Fun√ß√£o principal para an√°lise de feature importance
# -----------------------------------

def feature_importance_analyzer(
    target: str,
    features: list,
    method: str = "linear",
    max_depth: int = 4,
    random_state: int = 42
) -> str:
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    if target not in df.columns:
        return json.dumps({"mensagem": f"Coluna alvo {target} n√£o encontrada.", "status": "error"})

    valid_features = [f for f in features if f in df.columns and pd.api.types.is_numeric_dtype(df[f])]
    if not valid_features:
        return json.dumps({"mensagem": "Nenhuma vari√°vel num√©rica v√°lida encontrada entre as features.", "status": "error"})

    X = df[valid_features].dropna()
    y = df.loc[X.index, target].dropna()
    X = X.loc[y.index]

    if method == "linear":
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        model = LinearRegression()
        model.fit(X_scaled, y)
        importances = dict(zip(valid_features, model.coef_))
        importances = {k: abs(v) / sum(abs(val) for val in importances.values()) for k, v in importances.items()}
    else:
        model = DecisionTreeRegressor(max_depth=max_depth, random_state=random_state)
        model.fit(X, y)
        importances = dict(zip(valid_features, model.feature_importances_))

    # Ordenar import√¢ncias
    sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)

    # üëâ Interpreta√ß√£o textual usando fun√ß√£o auxiliar
    mensagem = interpretar_feature_importance(sorted_imp, target, method)

    result = {
        "mensagem": mensagem,
        "acao": "feature_importance",
        "args": {
            "importances": sorted_imp,
            "target": target,
            "method": method
        },
        "status": "success"
    }
    return json.dumps(result)

# -----------------------------------
# Ferramentas auxiliares de frequencia_valores
# -----------------------------------

def interpretar_frequencias(freq: pd.Series, coluna: str, top_n: int = 5) -> str:
    """
    Gera interpreta√ß√£o textual para os valores mais e menos frequentes de uma coluna.
    """
    if freq.empty:
        return f"A coluna **{coluna}** n√£o possui valores v√°lidos."

    top_vals = freq.head(top_n)
    bottom_vals = freq.tail(top_n)

    mensagens = [f"An√°lise de frequ√™ncia da coluna **{coluna}**:"]

    # Destacar o valor mais frequente
    mais_comum, mais_freq = top_vals.index[0], top_vals.iloc[0]
    mensagens.append(f"O valor mais frequente foi **{mais_comum}** ({mais_freq} ocorr√™ncias).")

    # Listar top N
    mensagens.append("Valores mais frequentes:")
    for idx, val in top_vals.items():
        mensagens.append(f"- {idx}: {val} ocorr√™ncias")

    # Listar menos frequentes
    mensagens.append("Valores menos frequentes:")
    for idx, val in bottom_vals.items():
        mensagens.append(f"- {idx}: {val} ocorr√™ncias")

    return "\n".join(mensagens)

# -----------------------------------
# Fun√ß√£o principal para frequencia_valores
# -----------------------------------
def frequencia_valores(coluna: str, top_n: int = 5) -> str:
    """
    Analisa a frequ√™ncia de valores em uma coluna (categ√≥rica ou num√©rica discreta).
    Retorna os valores mais e menos frequentes.
    """
    df = st.session_state.get("df_pandas_sample")
    if df is None:
        return json.dumps({"mensagem": "Nenhuma amostra dispon√≠vel.", "status": "error"})

    if coluna not in df.columns:
        return json.dumps({"mensagem": f"Coluna {coluna} n√£o encontrada.", "status": "error"})

    freq = df[coluna].value_counts(dropna=False)

    if freq.empty:
        return json.dumps({"mensagem": f"A coluna {coluna} n√£o possui valores v√°lidos.", "status": "error"})

    # üëâ Interpreta√ß√£o textual usando fun√ß√£o auxiliar
    mensagem = interpretar_frequencias(freq, coluna, top_n=top_n)

    result = {
        "mensagem": mensagem,
        "acao": "frequencia_valores",
        "args": {
            "coluna": coluna,
            "top_n": top_n
        },
        "status": "success"
    }
    return json.dumps(result)

# -----------------------------------
# Lista de ferramentas
# -----------------------------------

tools_list = [
    StructuredTool.from_function(
        func=get_columns,
        name="colunas_dataframe",
        description="Lista as colunas dispon√≠veis no DataFrame.",
        args_schema=SemArgs
    ),
    StructuredTool.from_function(
        func=get_summary,
        name="resumo_estatistico",
        description="Mostra estat√≠sticas b√°sicas do DataFrame inteiro ou de uma coluna espec√≠fica.",
        args_schema=ColunaInput
    ),
    StructuredTool.from_function(
        func=get_data_types,
        name="tipos_dados",
        description="Informa os tipos de cada coluna, separando num√©ricas e categ√≥ricas.",
        args_schema=SemArgs
    ),
    StructuredTool.from_function(
        func=get_distributions,
        name="distribuicoes",
        description="Mostra distribui√ß√µes de valores: estat√≠sticas para num√©ricas, contagem para categ√≥ricas.",
        args_schema=ColunaInput
    ),
    StructuredTool.from_function(
        func=get_ranges,
        name="intervalos",
        description="Mostra m√≠nimo e m√°ximo das colunas num√©ricas.",
        args_schema=ColunaInput
    ),
    StructuredTool.from_function(
        func=get_central_tendency,
        name="tendencia_central",
        description="Calcula m√©dia, mediana e moda de uma coluna num√©rica ou de todas as colunas num√©ricas.",
        args_schema=ColunaInput
    ),
    StructuredTool.from_function(
        func=get_variability,
        name="variabilidade",
        description="Calcula desvio padr√£o e vari√¢ncia de uma coluna num√©rica ou de todas as colunas num√©ricas.",
        args_schema=ColunaInput
    ),
    StructuredTool.from_function(
    func=histogram_maker,
    name="histograma_interativo",
    description="Gera um dicion√°rio de informa√ß√µes que ser√£o usados para plotar um histograma pela interface Streamlit. Aceita par√¢metros para personaliza√ß√£o do gr√°fico.",
    args_schema=HistogramInput,
    return_direct=True
    ),
    StructuredTool.from_function(
    func=outlier_analyzer,
    name="outlier_analyzer",
    description=(
        "Analisa uma coluna num√©rica para identificar outliers usando a regra do IQR. "
        "Retorna um dicion√°rio contendo uma mensagem interpretativa e instru√ß√µes para "
        "plotar um boxplot na interface Streamlit. "
        "Aceita par√¢metros opcionais para personaliza√ß√£o do gr√°fico e controle da an√°lise "
        "(multiplicador do IQR, inclus√£o de resumo estat√≠stico e recomenda√ß√µes). "
        "Tamb√©m permite aplicar tratamentos especiais: remover outliers, aplicar transforma√ß√£o logar√≠tmica "
        "ou winsorizar os dados, sempre comparando estat√≠sticas com a vers√£o normal."),
    args_schema=OutlierAnalyzerInput,
    return_direct=True
    ),
    StructuredTool.from_function(
    func=scatterplot_maker,
    name="scatterplot",
    description=(
        "Gera um dicion√°rio de informa√ß√µes que ser√° usado para plotar um gr√°fico de dispers√£o "
        "(scatterplot) entre duas colunas num√©ricas na interface Streamlit. "
        "Al√©m do gr√°fico, calcula correla√ß√µes de Pearson e Spearman, detecta outliers bivariados "
        "e avalia heterocedasticidade, retornando uma interpreta√ß√£o textual junto ao gr√°fico." ),
    args_schema=ScatterplotInput,
    return_direct=True
    ),
    StructuredTool.from_function(
    func=cluster_analyzer,
    name="cluster_analyzer",
    description=(
            "Executa an√°lise de clusteriza√ß√£o geral com KMeans em m√∫ltiplas vari√°veis num√©ricas. "
            "Aceita par√¢metros opcionais: max_clusters (padr√£o 6), random_state (padr√£o 42), "
            "min_cluster_size (descarta clusters pequenos), scaling ('standard', 'minmax' ou 'none'). "
            "Retorna interpreta√ß√£o textual e instru√ß√µes para plotar um heatmap de m√©dias por cluster."),
    args_schema=ClusterAnalyzerInput,
    return_direct=True
    ),
    StructuredTool.from_function(
    func=crosstab_analyzer,
    name="crosstab_analyzer",
    args_schema=CrosstabAnalyzerInput,
    description=(
        "Gera uma tabela cruzada entre duas vari√°veis categ√≥ricas. "
        "Pode retornar em formato de tabela ('table') ou como heatmap ('heatmap'). "
        "Se a cardinalidade for muito alta, for√ßa automaticamente o modo heatmap. "
        "Aceita par√¢metro normalize=True para mostrar propor√ß√µes em vez de contagens absolutas."),
    return_direct=True
    ),
    StructuredTool.from_function(
    func=correlation_matrix,
    name="correlation_matrix",
    args_schema=CorrelationMatrixInput,
    description=(
        "Calcula a matriz de correla√ß√£o entre todas as vari√°veis num√©ricas. "
        "Retorna interpreta√ß√£o textual destacando os pares mais correlacionados "
        "e instru√ß√µes para plotar um heatmap da matriz de correla√ß√£o."),
    return_direct=True
    ),
    StructuredTool.from_function(
        func=temporal_trends,
        name="temporal_trends",
        args_schema=TemporalTrendsInput,
        description=(
            "Analisa a evolu√ß√£o temporal de vari√°veis num√©ricas em rela√ß√£o a uma coluna de tempo. "
            "Permite escolher frequ√™ncia de agrega√ß√£o (di√°ria, mensal, anual) e fun√ß√£o de agrega√ß√£o (m√©dia, soma, etc.). "
            "Retorna interpreta√ß√£o textual e instru√ß√µes para plotar s√©ries temporais na interface Streamlit."),
        return_direct=True
    ),
    StructuredTool.from_function(
        func=feature_importance_analyzer,
        name="feature_importance_analyzer",
        args_schema=FeatureImportanceInput,
        description=(
            "Mede a import√¢ncia relativa de vari√°veis independentes para prever uma vari√°vel alvo. "
            "Pode usar regress√£o linear m√∫ltipla (coeficientes padronizados) ou √°rvore de decis√£o (feature importance). "
            "Retorna interpreta√ß√£o textual e instru√ß√µes para plotar gr√°fico de barras com as import√¢ncias."),
        return_direct=True
    ),
    StructuredTool.from_function(
        func=frequencia_valores,
        name="frequencia_valores",
        args_schema=FrequenciaValoresInput,
        description=(
            "Analisa a frequ√™ncia de valores em uma coluna categ√≥rica ou num√©rica discreta. "
            "Retorna os N valores mais frequentes e pode plotar um gr√°fico de barras."),
        return_direct=True
    ),

]


